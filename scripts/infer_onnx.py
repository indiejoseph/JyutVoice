#!/usr/bin/env python3
"""
ONNX-based inference script for JyutVoice TTS.

This script replicates the PyTorch inference pipeline using exported ONNX models,
enabling TTS synthesis without PyTorch dependencies.

Usage:
    python scripts/infer_onnx.py --text "Hello world" --ref_audio path/to/audio.wav --output output.wav
"""

import argparse
import time
import torch
import torchaudio
import onnxruntime as ort
import numpy as np
from hyperpyyaml import load_hyperpyyaml
from jyutvoice.text import text_to_sequence
from jyutvoice.utils.utils import intersperse
from pydips import BertModel
import torchaudio.compliance.kaldi as kaldi
from jyutvoice.utils.audio import mel_spectrogram


class ONNXInference:
    """ONNX-based TTS inference pipeline."""

    def __init__(self, config_path="configs/base.yaml", device="cpu"):
        self.device = device

        # Load configuration
        with open(config_path, "r") as f:
            configs = load_hyperpyyaml(f)

        # Load HiFT vocoder (still needs PyTorch)
        self.hift = configs["hift"].eval().to(device)
        hift_model = "pretrained_models/hift.pt"
        self.hift.load_state_dict(torch.load(hift_model, map_location=device))

        # Load ONNX models
        self.load_onnx_models()

        # Load auxiliary models
        self.load_auxiliary_models()

        # Initialize word segmentation model
        self.ws_model = BertModel()

    def load_onnx_models(self):
        """Load all ONNX models for inference."""
        option = ort.SessionOptions()
        option.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        option.intra_op_num_threads = 1

        # FlowEncoder for prompt hidden states
        self.flow_encoder_session = ort.InferenceSession(
            "pretrained_models/flow_encoder.onnx",
            sess_options=option,
            providers=["CPUExecutionProvider"],
        )

        # TextEncoderStyleEncoder for text encoding
        self.text_style_encoder_session = ort.InferenceSession(
            "pretrained_models/text_style_encoder.onnx",
            sess_options=option,
            providers=["CPUExecutionProvider"],
        )

        # FlowDecoder for mel generation
        self.flow_decoder_session = ort.InferenceSession(
            "pretrained_models/flow_decoder.onnx",
            sess_options=option,
            providers=["CPUExecutionProvider"],
        )

        print("‚úÖ All ONNX models loaded successfully!")

    def load_auxiliary_models(self):
        """Load auxiliary ONNX models for feature extraction."""
        option = ort.SessionOptions()
        option.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        option.intra_op_num_threads = 1

        # Speaker embedding model (CamppLUS)
        self.spk_model = ort.InferenceSession(
            "pretrained_models/campplus.onnx",
            sess_options=option,
            providers=["CPUExecutionProvider"],
        )

        # Speech tokenizer
        self.speech_tokenizer_session = ort.InferenceSession(
            "pretrained_models/speech_tokenizer_v2.onnx",
            sess_options=option,
            providers=["CPUExecutionProvider"],
        )

        print("‚úÖ Auxiliary models loaded successfully!")

    def word_seg(self, text: str):
        """Perform word segmentation for Chinese/Cantonese text."""
        text = self.ws_model.cut(text, mode="coarse")
        text = " ".join(text)
        return text

    def get_text(self, text: str, lang: str, phone: str = None):
        """Convert text to linguistic features."""
        if lang in ["zh", "yue"]:
            text = self.word_seg(text)

        phone_token_ids, tones, word_pos, syllable_pos, lang_ids = text_to_sequence(
            text, lang=lang, phone=phone
        )

        # Intersperse with zeros
        phone_token_ids = intersperse(phone_token_ids, 0)
        tones = intersperse(tones, 0)
        word_pos = intersperse(word_pos, 0)
        syllable_pos = intersperse(syllable_pos, 0)
        lang_ids = intersperse(lang_ids, 0)

        # Convert to tensors
        x = torch.tensor([phone_token_ids])
        x_lengths = torch.tensor([len(phone_token_ids)])
        tones = torch.tensor([tones])
        word_pos = torch.tensor([word_pos])
        syllable_pos = torch.tensor([syllable_pos])
        lang_ids = torch.tensor([lang_ids])

        return x, x_lengths, tones, word_pos, syllable_pos, lang_ids

    def extract_speech_feat(self, speech):
        """Extract mel spectrogram features from audio."""
        speech_feat = (
            mel_spectrogram(
                speech,
                n_fft=1920,
                num_mels=80,
                sampling_rate=24000,
                hop_size=480,
                win_size=1920,
                fmin=0,
                fmax=8000,
                center=False,
            )
            .squeeze(0)
            .transpose(0, 1)
            .unsqueeze(0)
        )  # [B, T, n_mel]

        speech_feat_len = torch.tensor([speech_feat.shape[1]])
        return speech_feat, speech_feat_len

    def extract_speech_token(self, audio):
        """Extract speech tokens from 16kHz audio."""
        # Ensure audio is numpy array
        if isinstance(audio, torch.Tensor):
            audio = audio.cpu().numpy()
        elif isinstance(audio, np.ndarray):
            pass
        else:
            raise ValueError("Audio must be torch.Tensor or numpy.ndarray")

        # Convert to torch tensor for mel-spectrogram
        audio_tensor = torch.from_numpy(audio).float().unsqueeze(0)

        # Compute mel spectrogram (whisper-style: 128 mel bins, 16kHz)
        feat = self._log_mel_spectrogram(
            audio_tensor, n_mels=128, n_fft=400, hop_length=160
        )

        # Run speech tokenizer
        speech_token = (
            self.speech_tokenizer_session.run(
                None,
                {
                    self.speech_tokenizer_session.get_inputs()[0]
                    .name: feat.detach()
                    .cpu()
                    .numpy(),
                    self.speech_tokenizer_session.get_inputs()[1].name: np.array(
                        [feat.shape[2]], dtype=np.int32
                    ),
                },
            )[0]
            .flatten()
            .tolist()
        )

        speech_token = torch.tensor([speech_token], dtype=torch.int32)
        speech_token_len = torch.tensor([len(speech_token[0])], dtype=torch.int32)

        return speech_token, speech_token_len

    def _log_mel_spectrogram(self, audio, n_mels=128, n_fft=400, hop_length=160):
        """Compute log mel spectrogram similar to whisper."""
        # Create mel spectrogram transform
        mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=16000,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            f_min=0,
            f_max=8000,
            norm="slaney",
            mel_scale="slaney",
        )

        # Compute mel spectrogram
        mel_spec = mel_transform(audio)

        # Convert to log scale
        log_mel_spec = torch.log(torch.clamp(mel_spec, min=1e-10))

        # Normalize (mean=0, std=1) like whisper does
        log_mel_spec = (log_mel_spec - log_mel_spec.mean()) / (
            log_mel_spec.std() + 1e-10
        )

        return log_mel_spec

    def extract_spk_embedding(self, speech):
        """Extract speaker embedding from audio."""
        feat = kaldi.fbank(speech, num_mel_bins=80, dither=0, sample_frequency=16000)
        spk_feat = feat - feat.mean(dim=0, keepdim=True)

        embedding = (
            self.spk_model.run(
                None,
                {
                    self.spk_model.get_inputs()[0]
                    .name: spk_feat.unsqueeze(dim=0)
                    .cpu()
                    .numpy()
                },
            )[0]
            .flatten()
            .tolist()
        )

        embedding = torch.tensor([embedding])
        return embedding

    def get_decoder_hidden_state_onnx(self, speech_token, speech_token_len):
        """Extract hidden state using ONNX FlowEncoder."""
        # Convert to numpy
        token_np = speech_token.detach().cpu().numpy().astype(np.int64)
        token_len_np = speech_token_len.detach().cpu().numpy().astype(np.int64)

        # Run ONNX inference
        inputs = {
            "token": token_np,
            "token_len": token_len_np,
        }

        outputs = self.flow_encoder_session.run(None, inputs)
        h, h_lengths = outputs

        # Convert back to torch tensor
        h = torch.from_numpy(h)
        h_lengths = torch.from_numpy(h_lengths)

        return h, h_lengths

    def text_style_encode_onnx(
        self, x, x_lengths, lang, tone, word_pos, syllable_pos, spk_embed, prompt_feat
    ):
        """Encode text and style using ONNX TextEncoderStyleEncoder."""
        # Convert to numpy
        x_np = x.detach().cpu().numpy().astype(np.int64)
        x_lengths_np = x_lengths.detach().cpu().numpy().astype(np.int64)
        lang_np = lang.detach().cpu().numpy().astype(np.int64)
        tone_np = tone.detach().cpu().numpy().astype(np.int64)
        word_pos_np = word_pos.detach().cpu().numpy().astype(np.int64)
        syllable_pos_np = syllable_pos.detach().cpu().numpy().astype(np.int64)
        spk_embed_np = spk_embed.detach().cpu().numpy().astype(np.float32)
        prompt_feat_np = prompt_feat.detach().cpu().numpy().astype(np.float32)

        # Run ONNX inference
        inputs = {
            "x": x_np,
            "x_lengths": x_lengths_np,
            "lang": lang_np,
            "tone": tone_np,
            "word_pos": word_pos_np,
            "syllable_pos": syllable_pos_np,
            "spk_embed": spk_embed_np,
            "prompt_feat": prompt_feat_np,
        }

        outputs = self.text_style_encoder_session.run(None, inputs)
        mu_x, logw, x_mask, style_cond, spk_embed_proj = outputs

        # Convert back to torch tensors
        mu_x = torch.from_numpy(mu_x)
        logw = torch.from_numpy(logw)
        x_mask = torch.from_numpy(x_mask)
        style_cond = torch.from_numpy(style_cond)
        spk_embed_proj = torch.from_numpy(spk_embed_proj)

        return mu_x, logw, x_mask, style_cond, spk_embed_proj

    def flow_decode_onnx(self, mu, mask, spks, cond, n_timesteps, temperature):
        """Generate mel spectrogram using ONNX FlowDecoder."""
        # Convert to numpy
        mu_np = mu.detach().cpu().numpy().astype(np.float32)
        mask_np = mask.detach().cpu().numpy().astype(np.float32)
        spks_np = spks.detach().cpu().numpy().astype(np.float32)
        cond_np = cond.detach().cpu().numpy().astype(np.float32)
        n_timesteps_np = np.array(n_timesteps, dtype=np.int64)
        temperature_np = np.array(temperature, dtype=np.float32)

        # Run ONNX inference
        inputs = {
            "mu": mu_np,
            "mask": mask_np,
            "spks": spks_np,
            "cond": cond_np,
            "n_timesteps": n_timesteps_np,
            "temperature": temperature_np,
        }

        outputs = self.flow_decoder_session.run(None, inputs)
        decoder_outputs = torch.from_numpy(outputs[0])

        return decoder_outputs

    def synthesise_onnx(
        self,
        x,
        x_lengths,
        lang,
        tone,
        word_pos,
        syllable_pos,
        prompt_feat,
        prompt_h,
        spk_embed,
        n_timesteps=10,
        length_scale=1.0,
    ):
        """Complete synthesis pipeline using ONNX models."""
        # 1. Text and style encoding
        mu_x, logw, x_mask, style_cond, spk_embed_proj = self.text_style_encode_onnx(
            x, x_lengths, lang, tone, word_pos, syllable_pos, spk_embed, prompt_feat
        )

        # 2. Prepare decoder inputs
        # Use logw for duration prediction (simplified - in real implementation you'd use duration predictor)
        # For now, use a simple length scaling
        text_len = x_lengths.item()
        mel_len = int(
            text_len * length_scale * 2
        )  # Rough estimate: ~2 mel frames per text token

        # Create mel mask
        mel_mask = torch.ones(1, 1, mel_len)

        # Expand mu_x to mel length (this is simplified - real alignment would use duration predictor)
        mu = mu_x.repeat(1, 1, mel_len // mu_x.shape[2] + 1)[:, :, :mel_len]

        # Use spk_embed_proj as speaker conditioning
        spks = spk_embed_proj

        # Use style_cond as conditioning
        cond = style_cond.unsqueeze(2).repeat(1, 1, mel_len)

        # 3. Flow decoding
        mel = self.flow_decode_onnx(mu, mel_mask, spks, cond, n_timesteps, 1.0)

        return {
            "mel": mel,
            "mel_lengths": torch.tensor([mel_len]),
            "attn": None,  # Not available in simplified ONNX version
            "rtf": 0.0,  # Not measured in this implementation
        }

    def infer(
        self,
        text,
        ref_audio_path,
        lang="yue",
        phone=None,
        n_timesteps=10,
        length_scale=0.9,
    ):
        """Complete inference from text and reference audio."""
        print(f"üîä Synthesizing: {text}")
        print(f"üìù Language: {lang}")
        print(f"üéµ Reference audio: {ref_audio_path}")

        start_time = time.time()

        # 1. Load and preprocess reference audio
        ref_audio, ref_sr = torchaudio.load(ref_audio_path)

        # Resample to 16kHz for tokenization and speaker embedding
        if ref_sr != 16000:
            resampler_16k = torchaudio.transforms.Resample(
                orig_freq=ref_sr, new_freq=16000
            )
            ref_audio_16k = resampler_16k(ref_audio)
        else:
            ref_audio_16k = ref_audio

        # Resample to 24kHz for mel spectrogram
        if ref_sr != 24000:
            resampler_24k = torchaudio.transforms.Resample(
                orig_freq=ref_sr, new_freq=24000
            )
            ref_audio_24k = resampler_24k(ref_audio)
        else:
            ref_audio_24k = ref_audio

        # 2. Extract reference features
        print("üéØ Extracting reference features...")
        prompt_feat, _ = self.extract_speech_feat(ref_audio_24k)
        prompt_token, prompt_tokens_len = self.extract_speech_token(
            ref_audio_16k.squeeze(0)
        )
        prompt_h, _ = self.get_decoder_hidden_state_onnx(
            prompt_token, prompt_tokens_len
        )
        spk_embed = self.extract_spk_embedding(ref_audio_16k)

        # 3. Process text
        print("üìù Processing text...")
        x, x_lengths, tones, word_pos, syllable_pos, lang_ids = self.get_text(
            text, lang, phone
        )

        # 4. Synthesize
        print("üéµ Synthesizing speech...")
        result = self.synthesise_onnx(
            x=x,
            x_lengths=x_lengths,
            lang=lang_ids,
            tone=tones,
            word_pos=word_pos,
            syllable_pos=syllable_pos,
            prompt_feat=prompt_feat,
            prompt_h=prompt_h,
            spk_embed=spk_embed,
            n_timesteps=n_timesteps,
            length_scale=length_scale,
        )

        # 5. Convert mel to waveform using HiFT
        print("üîä Generating waveform...")
        with torch.no_grad():
            wav, _ = self.hift.inference(result["mel"])

        end_time = time.time()
        synthesis_time = end_time - start_time

        print(f"‚è±Ô∏è  Synthesis time: {synthesis_time:.2f} seconds")
        print(f"üéµ Mel shape: {result['mel'].shape}")
        print(f"üîä Waveform shape: {wav.shape}")

        return wav, result


def main():
    parser = argparse.ArgumentParser(description="ONNX-based JyutVoice TTS Inference")
    parser.add_argument("--text", type=str, required=True, help="Text to synthesize")
    parser.add_argument(
        "--ref_audio", type=str, required=True, help="Path to reference audio file"
    )
    parser.add_argument(
        "--output", type=str, default="output.wav", help="Output audio file path"
    )
    parser.add_argument(
        "--lang",
        type=str,
        default="yue",
        choices=["en", "zh", "yue", "multilingual"],
        help="Language of the text",
    )
    parser.add_argument(
        "--phone", type=str, default=None, help="Phonetic input (optional)"
    )
    parser.add_argument(
        "--n_timesteps", type=int, default=10, help="Number of diffusion steps"
    )
    parser.add_argument(
        "--length_scale", type=float, default=0.9, help="Speech speed control"
    )
    parser.add_argument(
        "--config", type=str, default="configs/base.yaml", help="Config file path"
    )

    args = parser.parse_args()

    # Initialize ONNX inference
    print("üöÄ Initializing ONNX inference pipeline...")
    inference = ONNXInference(config_path=args.config)

    # Run inference
    wav, result = inference.infer(
        text=args.text,
        ref_audio_path=args.ref_audio,
        lang=args.lang,
        phone=args.phone,
        n_timesteps=args.n_timesteps,
        length_scale=args.length_scale,
    )

    # Save output
    torchaudio.save(args.output, wav.cpu(), 24000)
    print(f"üíæ Saved synthesized audio to: {args.output}")

    print("‚úÖ Inference completed successfully!")


if __name__ == "__main__":
    main()
